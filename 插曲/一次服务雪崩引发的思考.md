# 一次服务雪崩引发的思考

#### 背景交代

一个服务在A地区已经上线，且稳定提供服务两个月+，B地区因为客户端的Bug问题，被动上线该服务，但是B地区没有开放接口，导致请求失败，用户无法获取数据。产品要求在地区B上线该服务。我观察过地区A该服务的`QPS`，峰值时165。而且B地区 `DAU` < A地区，经过评估，感觉同样的服务器配置，服务上线没有问题。准备上线

##### *刺激的事情就要来了*

经过各项准备，项目已经可以上线。我就开始了上线。上线完毕，观察服务器的部署日志，正常。查看服务器的运行状态，正常。简直就是完美。此时流量开始进来。
> 但是

打开服务器运行日志`tail -f `， 突然发现异常日志开始刷屏
> 心里开始慌了

打开与该服务有依赖关系的`CMS`项目，发现服务不可用。
> 卧槽，万马奔腾啊

*敏感的自己感觉到事情没有这么简单了，开始回滚服务*
服务还没有回滚完成，手机上的报警短信开始刷屏，微信群组开始炸开了锅。
``` 
服务崩了 
```
数据库慢查询日志陡增，异常全都是超时，初步判断服务瓶颈出现在数据库。

> 我在群中大吼一声，稳住，我已经回滚服务，请 `DBA` 帮忙观察 Mongo 可用度，哈哈

话虽这样说，但是心里紧张的不得了，心想服务怎么回滚完成。真的是备受煎熬的5分钟。

##### 回滚完成。

DBA 开始在群里说服务现状
> 波峰开始下降
> 服务可用度开始升高
> 可以开始正常处理业务

##### 悬着的心开始落地了

> 请DBA帮忙收集一下线上DB日志吧，我来分析一下哪里出现问题了。

两个小时后，DB发来线上日志，发现同一个Query慢查询几十万条，找到语句后，在Mongo中做了explain，发现通过某个Id查询是全表扫描，`怎么不走索引`。观察后检查后发现同一个服务的不同地区数据库设置不对齐（索引创建没有对齐）。

> 尴尬

原因找到了，开始进一步处理问题。

## 思考
- 虽然上线之前做了服务压力相关的评估，但是还是被环境问题给吓了一大跳，怎么避免
- 是不是应该这这种案例下采取灰度上线的方式。（上线的服务先丢掉一部分请求，只处理少量请求，然后一步步的开放。避免上线后服务上线后直接挂掉）
- 多地区服务环境没有对齐，需要一个地方上线服务后，同步配置到其余地区
- 能不能迁移一部分数据库压力到业务层用缓存处理。